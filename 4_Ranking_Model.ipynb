{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import SamModel, SamConfig, SamProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the folders containing the patches and labels\n",
    "data_dir = \"/data3/animesh/vscode/Kaggle/dataset\"\n",
    "patches_dir = os.path.join(data_dir, \"training_patches\")\n",
    "labels_dir = os.path.join(data_dir, \"training_noisy_labels\")\n",
    "\n",
    "# Function to load all images in a directory\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = os.listdir(folder)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".png\"):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path)#.convert('RGB')  # Converting to RGB for consistency\n",
    "            img = np.array(img)  # Convert to numpy array\n",
    "            images.append(img)\n",
    "    return images, filenames\n",
    "\n",
    "# Load training patches and noisy labels\n",
    "patches, patch_filenames = load_images_from_folder(patches_dir)\n",
    "noisy_labels, label_filenames = load_images_from_folder(labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(patches)\n",
    "masks = np.array(noisy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the indices of non-empty masks\n",
    "valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n",
    "# Filter the image and mask arrays to keep only the non-empty pairs\n",
    "filtered_images = images[valid_indices]\n",
    "filtered_masks = masks[valid_indices]\n",
    "filtered_patch_filenames = [patch_filenames[i] for i in valid_indices]\n",
    "filtered_label_filenames = [label_filenames[i] for i in valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (4836, 256, 256, 3)\n",
      "Mask shape: (4836, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"Image shape:\", filtered_images.shape) \n",
    "print(\"Mask shape:\", filtered_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "dataset_dict = {\n",
    "    \"image\": [Image.fromarray(img) for img in filtered_images],\n",
    "    \"label\": [Image.fromarray(mask) for mask in filtered_masks],\n",
    "}\n",
    "\n",
    "# Create the dataset using the datasets.Dataset class\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 4836\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>iou_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_11_86_0_836.png</td>\n",
       "      <td>0.360032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_12_78_836_1044.png</td>\n",
       "      <td>0.433616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_31_59_418_627.png</td>\n",
       "      <td>0.479887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_19_79_627_836.png</td>\n",
       "      <td>0.736264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_7_95_0_836.png</td>\n",
       "      <td>0.664070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                imageid  iou_score\n",
       "0     1_11_86_0_836.png   0.360032\n",
       "1  1_12_78_836_1044.png   0.433616\n",
       "2   0_31_59_418_627.png   0.479887\n",
       "3   1_19_79_627_836.png   0.736264\n",
       "4      1_7_95_0_836.png   0.664070"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/data3/animesh/vscode/Kaggle/IOU_epoch_10_filtered.csv', index_col= 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = list(zip(dataset['image'], dataset['label'], df['iou_score']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label, iou_score = self.data[idx]\n",
    "        # Convert image and label to appropriate PyTorch tensors\n",
    "        # ...\n",
    "        return image, label, iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PngImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PngImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4836, 3, 256, 256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(filtered_images).permute(0, 3, 1, 2).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4836, 1, 256, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(filtered_masks).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4836, 4, 256, 256])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.cat([torch.tensor(filtered_images).permute(0, 3, 1, 2).squeeze(), torch.tensor(filtered_masks).unsqueeze(1)], dim=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4836])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = torch.tensor(df['iou_score'].values, dtype = torch.float32)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>iou_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_11_86_0_836.png</td>\n",
       "      <td>0.360032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_12_78_836_1044.png</td>\n",
       "      <td>0.433616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_31_59_418_627.png</td>\n",
       "      <td>0.479887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_19_79_627_836.png</td>\n",
       "      <td>0.736264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_7_95_0_836.png</td>\n",
       "      <td>0.664070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                imageid  iou_score\n",
       "0     1_11_86_0_836.png   0.360032\n",
       "1  1_12_78_836_1044.png   0.433616\n",
       "2   0_31_59_418_627.png   0.479887\n",
       "3   1_19_79_627_836.png   0.736264\n",
       "4      1_7_95_0_836.png   0.664070"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InputImageData(images, masks):\n",
    "    image = images/255.0\n",
    "    image =  np.array(image, dtype = np.float32)\n",
    "    image = torch.tensor(image).permute(2, 0, 1).squeeze()\n",
    "\n",
    "    mask = np.array(masks, dtype = np.float32)\n",
    "    mask =  torch.tensor(mask).unsqueeze(0)\n",
    "\n",
    "    data = torch.cat([image, mask], dim=0)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = []\n",
    "for i in range(len(filtered_images)):\n",
    "    image = filtered_images[i]\n",
    "    mask = filtered_masks[i]\n",
    "    data = InputImageData(image, mask)\n",
    "    bucket.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4836"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomImageScoreDataset(Dataset):\n",
    "    def __init__(self, image_list, iou_scores_df):\n",
    "        self.image_list = image_list\n",
    "        self.iou_scores_df = iou_scores_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iou_scores_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the image (tensor) and IOU score for the given index\n",
    "        image = self.image_list[idx]  # shape: [4, 256, 256]\n",
    "        iou_score = self.iou_scores_df[idx]\n",
    "        \n",
    "        return image, iou_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomImageScoreDataset(bucket, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the dataset: 4836\n",
      "shape of the image: torch.Size([4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Check the length of the dataset\n",
    "print(f\"Total number of samples in the dataset: {len(custom_dataset)}\")\n",
    "print(f\"shape of the image: {custom_dataset[0][0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1e6de60ee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1441, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Test the DataLoader\n",
    "for i, (images, iou_score) in enumerate(custom_dataset):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    # print(f\"Images shape: {images.shape}\")  # Should be [batch_size, 4, 256, 256]\n",
    "    # print(f\"Scores: {scores}\")\n",
    "    break  # Just to check the first batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2747, in __getitems__\n    n_examples = len(batch[next(iter(batch))])\nTypeError: only integer tensors of a single element can be converted to an index\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_features\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1344\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2747, in __getitems__\n    n_examples = len(batch[next(iter(batch))])\nTypeError: only integer tensors of a single element can be converted to an index\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(data_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
