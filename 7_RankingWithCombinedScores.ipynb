{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>auto_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_12_79_836_627.tif</td>\n",
       "      <td>0.342461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_16_71_418_836.tif</td>\n",
       "      <td>0.207412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_8_85_209_627.tif</td>\n",
       "      <td>0.148328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_8_87_836_0.tif</td>\n",
       "      <td>0.299235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_24_67_0_209.tif</td>\n",
       "      <td>0.208693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               imageid  auto_score\n",
       "0  1_12_79_836_627.tif    0.342461\n",
       "1  1_16_71_418_836.tif    0.207412\n",
       "2   1_8_85_209_627.tif    0.148328\n",
       "3     1_8_87_836_0.tif    0.299235\n",
       "4    0_24_67_0_209.tif    0.208693"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/data3/animesh/vscode/Kaggle/auto_segmentation_scores.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1264, 2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>iou_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_11_86_0_836.png</td>\n",
       "      <td>0.962723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_12_78_836_1044.png</td>\n",
       "      <td>0.987991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_31_59_418_627.png</td>\n",
       "      <td>0.902557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_19_79_627_836.png</td>\n",
       "      <td>0.494019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_7_95_0_836.png</td>\n",
       "      <td>0.951294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                imageid  iou_score\n",
       "0     1_11_86_0_836.png   0.962723\n",
       "1  1_12_78_836_1044.png   0.987991\n",
       "2   0_31_59_418_627.png   0.902557\n",
       "3   1_19_79_627_836.png   0.494019\n",
       "4      1_7_95_0_836.png   0.951294"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/data3/animesh/vscode/Kaggle/sam2_agreement_ratio_scores.csv', index_col= None)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   imageid    5000 non-null   object \n",
      " 1   iou_score  5000 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 78.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extensions from imageid in both DataFrames\n",
    "df1['imageid'] = df1['imageid'].str.replace('.tif', '', regex=False)\n",
    "df2['imageid'] = df2['imageid'].str.replace('.png', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imageid</th>\n",
       "      <th>iou_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_11_86_0_836</td>\n",
       "      <td>0.962723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_12_78_836_1044</td>\n",
       "      <td>0.987991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_31_59_418_627</td>\n",
       "      <td>0.902557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_19_79_627_836</td>\n",
       "      <td>0.494019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_7_95_0_836</td>\n",
       "      <td>0.951294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0_19_13_836_627</td>\n",
       "      <td>0.625275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0_15_3_627_209</td>\n",
       "      <td>0.956863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0_32_69_209_0</td>\n",
       "      <td>0.984070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>1_25_78_836_836</td>\n",
       "      <td>0.039642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0_15_3_836_1044</td>\n",
       "      <td>0.992752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>668 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              imageid  iou_score\n",
       "0       1_11_86_0_836   0.962723\n",
       "1    1_12_78_836_1044   0.987991\n",
       "2     0_31_59_418_627   0.902557\n",
       "3     1_19_79_627_836   0.494019\n",
       "4        1_7_95_0_836   0.951294\n",
       "..                ...        ...\n",
       "663   0_19_13_836_627   0.625275\n",
       "664    0_15_3_627_209   0.956863\n",
       "665     0_32_69_209_0   0.984070\n",
       "666   1_25_78_836_836   0.039642\n",
       "667   0_15_3_836_1044   0.992752\n",
       "\n",
       "[668 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_filtered = df2[df2[\"imageid\"].isin(df1['imageid'].values)]\n",
    "df2_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               imageid  auto_score  iou_score  combined_score\n",
      "0      1_12_79_836_627    0.342461   0.806061        0.276044\n",
      "1      1_16_71_418_836    0.207412   0.906876        0.188097\n",
      "2       1_8_85_209_627    0.148328   0.934448        0.138604\n",
      "3         1_8_87_836_0    0.299235   0.833801        0.249503\n",
      "4        0_24_67_0_209    0.208693   0.993835        0.207406\n",
      "...                ...         ...        ...             ...\n",
      "2279  0_19_66_1044_836    0.101182   0.133347        0.013492\n",
      "2280     1_12_79_418_0    0.333739   0.827881        0.276297\n",
      "2281    1_13_83_0_1044    0.224008   0.913025        0.204524\n",
      "2282     2_20_41_836_0    0.131080   0.967331        0.126798\n",
      "2283      1_7_97_0_418    0.300350   0.926468        0.278264\n",
      "\n",
      "[2284 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove extensions from imageid in both DataFrames\n",
    "df1['imageid'] = df1['imageid'].str.replace('.tif', '', regex=False)\n",
    "df2['imageid'] = df2['imageid'].str.replace('.png', '', regex=False)\n",
    "\n",
    "# Perform the left merge based on the cleaned imageid columns\n",
    "df_combined = pd.merge(df1, df2, on=\"imageid\", how=\"left\")\n",
    "\n",
    "# Create the 'combined_score' column by multiplying 'auto_score' and 'iou_score'\n",
    "df_combined['combined_score'] = df_combined['auto_score'] * df_combined['iou_score']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_score = df_combined['iou_score'] * (1 + 0.1 * df_combined['auto_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               imageid  auto_score  iou_score  combined_score\n",
      "0      1_12_79_836_627    0.342461   0.806061        0.834250\n",
      "1      1_16_71_418_836    0.207412   0.906876        0.903014\n",
      "2       1_8_85_209_627    0.148328   0.934448        0.917391\n",
      "3         1_8_87_836_0    0.299235   0.833801        0.851958\n",
      "4        0_24_67_0_209    0.208693   0.993835        0.985882\n",
      "...                ...         ...        ...             ...\n",
      "2279  0_19_66_1044_836    0.101182   0.133347        0.146916\n",
      "2280     1_12_79_418_0    0.333739   0.827881        0.853235\n",
      "2281    1_13_83_0_1044    0.224008   0.913025        0.912175\n",
      "2282     2_20_41_836_0    0.131080   0.967331        0.945180\n",
      "2283      1_7_97_0_418    0.300350   0.926468        0.940214\n",
      "\n",
      "[2284 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove extensions from imageid in both DataFrames\n",
    "df1['imageid'] = df1['imageid'].str.replace('.tif', '', regex=False)\n",
    "df2['imageid'] = df2['imageid'].str.replace('.png', '', regex=False)\n",
    "\n",
    "# Perform the left merge based on the cleaned imageid columns\n",
    "df_combined = pd.merge(df1, df2, on=\"imageid\", how=\"left\")\n",
    "\n",
    "# Create the 'combined_score' column by multiplying 'auto_score' and 'iou_score'\n",
    "df_combined['combined_score'] = 0.95* df_combined['iou_score'] + 0.2 * df_combined['auto_score']\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches array shape: (2284, 256, 256, 3)\n",
      "Labels array shape: (2284, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to the folders containing the patches and labels\n",
    "data_dir = \"/data3/animesh/vscode/Kaggle/dataset\"\n",
    "patches_dir = os.path.join(data_dir, \"training_patches\")\n",
    "labels_dir = os.path.join(data_dir, \"training_noisy_labels\")\n",
    "\n",
    "# Assuming df_combined is already defined with an 'imageid' column containing the image names without extensions\n",
    "# Convert the 'imageid' column to a set of filenames for efficient lookup\n",
    "image_ids = set(df_combined['imageid'])\n",
    "\n",
    "# Initialize lists to store patch and label images as numpy arrays\n",
    "patches = []\n",
    "labels = []\n",
    "\n",
    "# Loop through the files in the patches directory\n",
    "for filename in os.listdir(patches_dir):\n",
    "    # Extract the filename without extension\n",
    "    image_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Check if this image_name is in the image_ids set and the file is a .png\n",
    "    if image_name in image_ids and filename.endswith(\".png\"):\n",
    "        # Construct the full file paths\n",
    "        patch_path = os.path.join(patches_dir, filename)\n",
    "        label_path = os.path.join(labels_dir, filename)  # Same filename for labels\n",
    "        \n",
    "        # Check if the corresponding label exists\n",
    "        if os.path.exists(label_path):\n",
    "            # Read the patch and label images\n",
    "            patch_img = np.array(Image.open(patch_path))\n",
    "            label_img = np.array(Image.open(label_path))\n",
    "            \n",
    "            # Append them to the lists\n",
    "            patches.append(patch_img)\n",
    "            labels.append(label_img)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "images = np.array(patches)\n",
    "masks = np.array(labels)\n",
    "\n",
    "# Display the shapes of the arrays\n",
    "print(\"Patches array shape:\", images.shape)\n",
    "print(\"Labels array shape:\", masks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset Class\n",
    "class ImageMaskScoreDataset(Dataset):\n",
    "    def __init__(self, images, masks, scores, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.scores = scores\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        score = self.scores[idx]\n",
    "\n",
    "        # Normalize image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2, 0, 1), \\\n",
    "               torch.tensor(mask, dtype=torch.float32).unsqueeze(0), \\\n",
    "               torch.tensor(score, dtype=torch.float32)\n",
    "\n",
    "# Splitting dataset into train, val, and test\n",
    "def create_dataloaders(images, masks, scores, batch_size=32, test_size=0.2, val_size=0.1):\n",
    "    # Train-test split\n",
    "    train_indices, test_indices, y_train, y_test = train_test_split(\n",
    "        np.arange(len(images)), scores, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Further split train into train-validation\n",
    "    train_size = int((1 - val_size) * len(train_indices))\n",
    "    val_size = len(train_indices) - train_size\n",
    "    train_indices, val_indices = train_indices[:train_size], train_indices[train_size:]\n",
    "\n",
    "    \n",
    "    \n",
    "    transform = lambda x: (x / 255.0)\n",
    "\n",
    "    # Creating datasets\n",
    "    train_dataset = ImageMaskScoreDataset(images[train_indices], masks[train_indices], y_train[:train_size], transform=transform)\n",
    "    val_dataset = ImageMaskScoreDataset(images[val_indices], masks[val_indices], y_train[train_size:], transform=transform)\n",
    "    test_dataset = ImageMaskScoreDataset(images[test_indices], masks[test_indices], y_test, transform=transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_loader, val_loader, test_loader = create_dataloaders(images, masks, df_combined['combined_score'].values, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: torch.Size([8, 3, 256, 256])\n",
      "Masks batch shape: torch.Size([8, 1, 256, 256])\n",
      "Scores batch shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# First batch from the train_loader\n",
    "for images, masks, scores in train_loader:\n",
    "    print(f\"Images batch shape: {images.shape}\")  # Shape of images in a batch\n",
    "    print(f\"Masks batch shape: {masks.shape}\")    # Shape of masks in a batch\n",
    "    print(f\"Scores batch shape: {scores.shape}\")  # Shape of scores in a batch\n",
    "    break  # Check only the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetSmall(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetSmall, self).__init__()\n",
    "\n",
    "        # Encoder with fewer channels\n",
    "        self.enc_conv1 = self.double_conv(4, 16)  # Reduced channels\n",
    "        self.enc_conv2 = self.double_conv(16, 32)\n",
    "        self.enc_conv3 = self.double_conv(32, 64)\n",
    "        self.enc_conv4 = self.double_conv(64, 128)\n",
    "\n",
    "        # Decoder with corresponding channel reductions\n",
    "        self.up_conv3 = self.up_conv(128, 64)\n",
    "        self.dec_conv3 = self.double_conv(128, 64)\n",
    "        self.up_conv2 = self.up_conv(64, 32)\n",
    "        self.dec_conv2 = self.double_conv(64, 32)\n",
    "        self.up_conv1 = self.up_conv(32, 16)\n",
    "        self.dec_conv1 = self.double_conv(32, 16)\n",
    "\n",
    "        # Adaptive pooling to match output shape for fully connected layer\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))  # Adjust to final output size as needed\n",
    "\n",
    "        # Final output layer for score prediction\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 8 * 8, 64),  # Adjusted for reduced feature size from pooling\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Single output for score prediction\n",
    "        )\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def up_conv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, images, masks):\n",
    "        # Concatenate images and masks along channel dimension\n",
    "        x = torch.cat((images, masks), dim=1)  # (B, 4, 256, 256)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc_conv1(x)\n",
    "        e2 = self.enc_conv2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc_conv3(F.max_pool2d(e2, 2))\n",
    "        e4 = self.enc_conv4(F.max_pool2d(e3, 2))\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.up_conv3(e4)\n",
    "        d3 = self.dec_conv3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.up_conv2(d3)\n",
    "        d2 = self.dec_conv2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.up_conv1(d2)\n",
    "        d1 = self.dec_conv1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        # Adaptive pooling to resize output for the fully connected layer\n",
    "        d1_pooled = self.pool(d1)\n",
    "\n",
    "        # Final fully connected layer to predict the score\n",
    "        out = self.final_fc(d1_pooled)\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "model = UNetSmall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNetSmallImproved(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetSmallImproved, self).__init__()\n",
    "\n",
    "        # Encoder with BatchNorm and LeakyReLU\n",
    "        self.enc_conv1 = self.double_conv(4, 32)  # Increased initial channels slightly\n",
    "        self.enc_conv2 = self.double_conv(32, 64)\n",
    "        self.enc_conv3 = self.double_conv(64, 128)\n",
    "        self.enc_conv4 = self.double_conv(128, 256)\n",
    "\n",
    "        # Decoder with corresponding channel scaling\n",
    "        self.up_conv3 = self.up_conv(256, 128)\n",
    "        self.dec_conv3 = self.double_conv(256, 128)\n",
    "        self.up_conv2 = self.up_conv(128, 64)\n",
    "        self.dec_conv2 = self.double_conv(128, 64)\n",
    "        self.up_conv1 = self.up_conv(64, 32)\n",
    "        self.dec_conv1 = self.double_conv(64, 32)\n",
    "\n",
    "        # Adaptive pooling to match output shape for fully connected layer\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4, 4))  # Adjusted pooling size\n",
    "\n",
    "        # Fully connected layers with dropout\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization\n",
    "            nn.LeakyReLU(0.1, inplace=True),  # LeakyReLU activation\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "\n",
    "    def up_conv(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, images, masks):\n",
    "        # Concatenate images and masks along channel dimension\n",
    "        x = torch.cat((images, masks), dim=1)  # (B, 4, 256, 256)\n",
    "\n",
    "        # Encoder\n",
    "        e1 = self.enc_conv1(x)\n",
    "        e2 = self.enc_conv2(F.max_pool2d(e1, 2))\n",
    "        e3 = self.enc_conv3(F.max_pool2d(e2, 2))\n",
    "        e4 = self.enc_conv4(F.max_pool2d(e3, 2))\n",
    "\n",
    "        # Decoder\n",
    "        d3 = self.up_conv3(e4)\n",
    "        d3 = self.dec_conv3(torch.cat([d3, e3], dim=1))\n",
    "        d2 = self.up_conv2(d3)\n",
    "        d2 = self.dec_conv2(torch.cat([d2, e2], dim=1))\n",
    "        d1 = self.up_conv1(d2)\n",
    "        d1 = self.dec_conv1(torch.cat([d1, e1], dim=1))\n",
    "\n",
    "        # Adaptive pooling to resize output for the fully connected layer\n",
    "        d1_pooled = self.pool(d1)\n",
    "\n",
    "        # Final fully connected layer to predict the score\n",
    "        out = self.final_fc(d1_pooled)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Instantiate the improved model\n",
    "model = UNetSmallImproved()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/animesh/vscode/Kaggle/venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/10:   0%|          | 0/206 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:59<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/10\n",
      "Mean Train Loss: 0.2301\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:59<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2/10\n",
      "Mean Train Loss: 0.0811\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:38<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3/10\n",
      "Mean Train Loss: 0.0653\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:19<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4/10\n",
      "Mean Train Loss: 0.0661\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [10:00<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5/10\n",
      "Mean Train Loss: 0.0650\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [09:47<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6/10\n",
      "Mean Train Loss: 0.0641\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [09:47<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7/10\n",
      "Mean Train Loss: 0.0605\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [09:46<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8/10\n",
      "Mean Train Loss: 0.0600\n",
      "Learning_rate:0.0001\n",
      "Mean Validation Loss: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [09:48<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9/10\n",
      "Mean Train Loss: 0.0613\n",
      "Learning_rate:1e-05\n",
      "Mean Validation Loss: 0.0356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 206/206 [09:43<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10/10\n",
      "Mean Train Loss: 0.0589\n",
      "Learning_rate:1e-05\n",
      "Mean Validation Loss: 0.0387\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "# Define optimizer and criterion\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss()\n",
    "def train_model(model, train_loader, val_loader, optimizer = optimizer, criterion = criterion, num_epochs=10):\n",
    "    # Set up learning rate scheduler (ReduceLROnPlateau)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_losses = []\n",
    "\n",
    "        # Progress bar for the training phase\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = batch[0].to(device)  # Assuming images are at index 0\n",
    "            masks = batch[1].to(device)   # Assuming masks are at index 1\n",
    "            scores = batch[2].to(device)  # Assuming scores are at index 2\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images, masks).squeeze()  # Squeeze the outputs to match the shape of scores\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, scores)\n",
    "\n",
    "            # Backward pass (compute gradients of parameters w.r.t. loss)\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()  # Backpropagation to compute gradients\n",
    "\n",
    "            # Optimize parameters (correctly using optimizer.step())\n",
    "            optimizer.step()  # Update weights, avoid calling optimizer()\n",
    "\n",
    "            # Record loss for the current batch\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        # Print mean loss for the epoch\n",
    "        mean_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        print(f'EPOCH: {epoch+1}/{num_epochs}')\n",
    "        print(f'Mean Train Loss: {mean_epoch_loss:.4f}')\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f'Learning_rate:{lr}')\n",
    "\n",
    "        # Validation phase\n",
    "        val_losses = []\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for batch in val_loader:\n",
    "                images = batch[0].to(device)\n",
    "                masks = batch[1].to(device)\n",
    "                scores = batch[2].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images, masks).squeeze()\n",
    "\n",
    "                # Compute validation loss\n",
    "                val_loss = criterion(outputs, scores)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        mean_val_loss = sum(val_losses) / len(val_losses)\n",
    "        print(f'Mean Validation Loss: {mean_val_loss:.4f}')\n",
    "\n",
    "        # Step the learning rate scheduler based on validation loss\n",
    "        scheduler.step(mean_val_loss)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the folders containing the patches and labels\n",
    "data_dir = \"/data3/animesh/vscode/Kaggle/dataset\"\n",
    "patches_dir = os.path.join(data_dir, \"training_patches\")\n",
    "labels_dir = os.path.join(data_dir, \"training_noisy_labels\")\n",
    "\n",
    "# Function to load all images in a directory\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = os.listdir(folder)\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".png\"):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path)#.convert('RGB')  # Converting to RGB for consistency\n",
    "            img = np.array(img)  # Convert to numpy array\n",
    "            images.append(img)\n",
    "    return images, filenames\n",
    "\n",
    "# Load training patches and noisy labels\n",
    "patches, patch_filenames = load_images_from_folder(patches_dir)\n",
    "noisy_labels, label_filenames = load_images_from_folder(labels_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(patches)\n",
    "masks = np.array(noisy_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Function to normalize the images using calculated mean and std\n",
    "def apply_normalization(images):\n",
    "    # Normalize the images (per channel)\n",
    "    return (images / 255.0)\n",
    "\n",
    "# Normalize the images based on the entire dataset\n",
    "def prepare_normalized_data(images, masks):\n",
    "   \n",
    "    # Apply normalization to the images\n",
    "    normalized_images = apply_normalization(images)\n",
    "\n",
    "    # Convert images and masks to PyTorch tensors\n",
    "    normalized_images = torch.tensor(normalized_images).permute(0, 3, 1, 2).float()  # [batch, channels, height, width]\n",
    "    masks = torch.tensor(masks).unsqueeze(1).float()  # [batch, 1, height, width]\n",
    "\n",
    "    return normalized_images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_images, masks = prepare_normalized_data(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Function to create a DataLoader\n",
    "def create_dataloader(images, masks, batch_size=32, shuffle=False):\n",
    "    # Create a TensorDataset with images and masks\n",
    "    dataset = TensorDataset(images, masks)\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume normalized_images and normalized_masks are prepared from the previous step\n",
    "dataloader = create_dataloader(normalized_images, masks, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                imageid  iou_score\n",
      "0     1_11_86_0_836.png   0.816832\n",
      "1  1_12_78_836_1044.png   0.779570\n",
      "2   0_31_59_418_627.png   0.815308\n",
      "3   1_19_79_627_836.png   0.805756\n",
      "4      1_7_95_0_836.png   0.823522\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Function to predict scores and store them in a DataFrame\n",
    "def predict_and_store_scores(model, dataloader, patch_filenames, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    \n",
    "    # Iterate through the dataloader and predict scores\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            images = batch[0].to(device)  # Load images to the device\n",
    "            masks = batch[1].to(device)   # Load masks to the device\n",
    "            \n",
    "            # Ensure that both images and masks have the same batch size\n",
    "            if images.shape[0] != masks.shape[0]:\n",
    "                raise ValueError(f\"Batch size mismatch: images batch {images.shape[0]} and masks batch {masks.shape[0]}\")\n",
    "            \n",
    "            # Predict scores\n",
    "            predicted_scores = model(images, masks)  # Assuming model takes both images and masks\n",
    "            predicted_scores = predicted_scores.squeeze(1)  # Ensure output is [batch_size, 1] and squeeze to [batch_size]\n",
    "            \n",
    "            # Move predicted scores to CPU and convert to NumPy\n",
    "            predicted_scores = predicted_scores.cpu().numpy()\n",
    "            predictions.extend(predicted_scores)\n",
    "\n",
    "    # Create a DataFrame to store imageid and iou_score\n",
    "    df_predictions = pd.DataFrame({\n",
    "        'imageid': patch_filenames,  # Image IDs from patch_filenames\n",
    "        'iou_score': predictions      # Predicted IoU scores\n",
    "    })\n",
    "\n",
    "    return df_predictions\n",
    "\n",
    "# Example usage:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming model and dataloader are prepared, and patch_filenames is a list of filenames\n",
    "df_results = predict_and_store_scores(model, dataloader, patch_filenames, device)\n",
    "\n",
    "# Save the DataFrame to a CSV file if needed\n",
    "# df_results.to_csv('predicted_iou_scores.csv', index=False)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 imageid  iou_score\n",
      "1733   1_14_72_0_627.png   1.077527\n",
      "1519  1_7_87_627_418.png   1.077338\n",
      "3281  1_6_87_836_418.png   1.056382\n",
      "3828    1_8_86_209_0.png   1.054950\n",
      "1937  1_7_87_627_627.png   1.045420\n"
     ]
    }
   ],
   "source": [
    "# Sort the DataFrame by IoU score (ascending order)\n",
    "df_sorted = df_results.sort_values(by='iou_score', ascending=False)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "print(df_sorted.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>imageid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1_14_72_0_627.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1_7_87_627_418.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1_6_87_836_418.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1_8_86_209_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1_7_87_627_627.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id             imageid\n",
       "0   0   1_14_72_0_627.png\n",
       "1   1  1_7_87_627_418.png\n",
       "2   2  1_6_87_836_418.png\n",
       "3   3    1_8_86_209_0.png\n",
       "4   4  1_7_87_627_627.png"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageid = list(df_sorted['imageid'])\n",
    "submission = pd.DataFrame({'id' : range(5000), 'imageid': imageid }, index = None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file if needed\n",
    "submission.to_csv('/data3/animesh/vscode/Kaggle/submission_33.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "False False\n",
      "False False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(patches_array).any(), np.isnan(labels_array).any())\n",
    "print(np.isinf(patches_array).any(), np.isinf(labels_array).any())\n",
    "print(np.isnan(df_combined['combined_score'].values).any(), np.isnan(df_combined['combined_score'].values).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with NaN or Inf values:\n",
      "              imageid  auto_score  iou_score  combined_score\n",
      "25    0_34_62_836_836    0.337333        NaN             NaN\n",
      "39      2_19_45_418_0    0.215769        NaN             NaN\n",
      "70   1_12_80_1044_627    0.437527        NaN             NaN\n",
      "78   2_15_45_1044_836    0.244235        NaN             NaN\n",
      "84     1_15_74_1044_0    0.388788        NaN             NaN\n",
      "155   0_30_63_209_836    0.335499        NaN             NaN\n",
      "171     2_26_34_0_836    0.343716        NaN             NaN\n",
      "207   1_12_79_418_836    0.403307        NaN             NaN\n",
      "209    1_9_97_836_627    0.373431        NaN             NaN\n",
      "211   0_32_58_418_209    0.269550        NaN             NaN\n",
      "267  1_5_100_1044_418    0.426051        NaN             NaN\n",
      "278    1_5_103_0_1044    0.397366        NaN             NaN\n",
      "329   0_17_67_836_418    0.276497        NaN             NaN\n",
      "351     1_21_79_836_0    0.351732        NaN             NaN\n",
      "359   0_18_19_627_418    0.327528        NaN             NaN\n",
      "397   1_7_87_418_1044    0.428951        NaN             NaN\n",
      "430   1_8_105_627_836    0.318958        NaN             NaN\n",
      "501   0_23_18_836_418    0.271853        NaN             NaN\n",
      "515  0_19_13_418_1044    0.456115        NaN             NaN\n",
      "551      1_9_99_627_0    0.356829        NaN             NaN\n",
      "553   0_13_2_1044_418    0.402150        NaN             NaN\n",
      "610     2_20_44_0_836    0.420246        NaN             NaN\n",
      "660   1_6_99_209_1044    0.427181        NaN             NaN\n",
      "662  0_37_61_1044_209    0.413076        NaN             NaN\n"
     ]
    }
   ],
   "source": [
    "# Select only numeric columns to check for NaN or Inf\n",
    "numeric_df = df_combined.select_dtypes(include=[np.number])\n",
    "\n",
    "# Find rows with NaN values\n",
    "nan_rows = df_combined[numeric_df.isna().any(axis=1)]\n",
    "\n",
    "# Find rows with Inf values (only on numeric columns)\n",
    "inf_rows = df_combined[(np.isinf(numeric_df)).any(axis=1)]\n",
    "\n",
    "# Combine the rows with NaN or Inf values for easier inspection\n",
    "nan_or_inf_rows = pd.concat([nan_rows, inf_rows]).drop_duplicates()\n",
    "\n",
    "# Display rows with NaN or Inf values\n",
    "print(\"Rows with NaN or Inf values:\")\n",
    "print(nan_or_inf_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              imageid  auto_score  iou_score  combined_score\n",
      "25    0_34_62_836_836    0.337333        NaN             NaN\n",
      "39      2_19_45_418_0    0.215769        NaN             NaN\n",
      "70   1_12_80_1044_627    0.437527        NaN             NaN\n",
      "78   2_15_45_1044_836    0.244235        NaN             NaN\n",
      "84     1_15_74_1044_0    0.388788        NaN             NaN\n",
      "155   0_30_63_209_836    0.335499        NaN             NaN\n",
      "171     2_26_34_0_836    0.343716        NaN             NaN\n",
      "207   1_12_79_418_836    0.403307        NaN             NaN\n",
      "209    1_9_97_836_627    0.373431        NaN             NaN\n",
      "211   0_32_58_418_209    0.269550        NaN             NaN\n",
      "267  1_5_100_1044_418    0.426051        NaN             NaN\n",
      "278    1_5_103_0_1044    0.397366        NaN             NaN\n",
      "329   0_17_67_836_418    0.276497        NaN             NaN\n",
      "351     1_21_79_836_0    0.351732        NaN             NaN\n",
      "359   0_18_19_627_418    0.327528        NaN             NaN\n",
      "397   1_7_87_418_1044    0.428951        NaN             NaN\n",
      "430   1_8_105_627_836    0.318958        NaN             NaN\n",
      "501   0_23_18_836_418    0.271853        NaN             NaN\n",
      "515  0_19_13_418_1044    0.456115        NaN             NaN\n",
      "551      1_9_99_627_0    0.356829        NaN             NaN\n",
      "553   0_13_2_1044_418    0.402150        NaN             NaN\n",
      "610     2_20_44_0_836    0.420246        NaN             NaN\n",
      "660   1_6_99_209_1044    0.427181        NaN             NaN\n",
      "662  0_37_61_1044_209    0.413076        NaN             NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_nan_rows(df):\n",
    "    \"\"\"\n",
    "    Function to find rows in a DataFrame that contain NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing rows with NaN values.\n",
    "    \"\"\"\n",
    "    # Select rows with NaN values\n",
    "    nan_rows = df[df.isna().any(axis=1)]\n",
    "    \n",
    "    return nan_rows\n",
    "\n",
    "# Example usage:\n",
    "nan_rows = find_nan_rows(df_combined)\n",
    "print(nan_rows)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
